{
  "stories": [
    {
      "by": "pingou",
      "descendants": 0,
      "id": 30975950,
      "score": 1,
      "time": 1649587453,
      "title": "How MBA-wielding bosses boost profits",
      "type": "story",
      "url": "https://www.economist.com/business/2022/04/09/how-mba-wielding-bosses-boost-profits"
    },
    {
      "by": "Tomte",
      "descendants": 0,
      "id": 30975890,
      "score": 1,
      "time": 1649586801,
      "title": "They Do the Work, You Reap the Yogurt (2009)",
      "type": "story",
      "url": "https://www.nytimes.com/2009/04/15/dining/15curi.html"
    },
    {
      "by": "Tomte",
      "descendants": 0,
      "id": 30975888,
      "score": 1,
      "time": 1649586775,
      "title": "Yog’s Law and Self-Publishing (2014)",
      "type": "story",
      "url": "https://whatever.scalzi.com/2014/06/20/yogs-law-and-self-publishing/"
    },
    {
      "by": "rcshubhadeep",
      "descendants": 0,
      "id": 30975887,
      "score": 1,
      "text": "I have been tinkering with some DL models and wanted to implement part of it using PyTorch einsum. Before doing so I was wondering about the performance. I have been a bit skeptic as I believe there is a parsing (and even may be somewhat code generation) involved in implementation of einsum (I have never look under the hood of PyTorch or Numpy as to how is it implemented, so I may be completely wrong)<p>So to measure the performance, I created a simple benchmark of comparison. I created a Tensor with these dimensions (BATCH, X, Y). Like so -<p>a = torch.randn(10, 20, 30)<p>Then in Jupyter I did this<p>%%timeit<p>torch.einsum(&#x27;b i j -&gt; b j i&#x27;, a)<p>AND<p>%%timeit<p>a.transpose(1, 2)<p>-----------------------<p>This is the result<p>5.43 µs ± 63.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each) [Einsum]<p>1.15 µs ± 2.51 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) [transpose]<p>Am I doing &#x2F; reading something wrong? Is it a wrong way to benchmark? Or is it really true what I see, that einsum is order of magnitude slower than transpose?",
      "time": 1649586769,
      "title": "Ask HN: Why PyTorch einsum is significantly slower than transpose",
      "type": "story"
    },
    {
      "by": "s1291",
      "descendants": 0,
      "id": 30975886,
      "score": 2,
      "time": 1649586766,
      "title": "NumPy-100",
      "type": "story",
      "url": "https://github.com/rougier/numpy-100"
    },
    {
      "by": "rntn",
      "descendants": 1,
      "id": 30975869,
      "kids": [
        30975878
      ],
      "score": 1,
      "time": 1649586552,
      "title": "‘They killed everyone.’ Fury in Ukraine at Russian troops’ barbarity",
      "type": "story",
      "url": "https://www.politico.eu/article/kill-fury-ukraine-russia-troop-barbarity/"
    },
    {
      "by": "rustoo",
      "descendants": 0,
      "id": 30975867,
      "score": 1,
      "time": 1649586489,
      "title": "Companies That Are Aggressive on Taxes Fall Short at Managing Their Workforce",
      "type": "story",
      "url": "https://news.ncsu.edu/2022/04/aggressive-taxes-labor-efficiency/"
    },
    {
      "by": "prakhargurunani",
      "descendants": 0,
      "id": 30975853,
      "score": 1,
      "time": 1649586290,
      "title": "How Reddit built the backend infrastructure for r/place – A short thread",
      "type": "story",
      "url": "https://twitter.com/FirePing32/status/1513099052166295552"
    },
    {
      "by": "takiwatanga",
      "descendants": 0,
      "id": 30975849,
      "score": 1,
      "time": 1649586241,
      "title": "Cathode Ray Television (1933)",
      "type": "story",
      "url": "http://www.r-type.org/articles/art-344.htm"
    },
    {
      "by": "kiyanwang",
      "descendants": 0,
      "id": 30975838,
      "score": 1,
      "time": 1649586109,
      "title": "Is Platform Engineering the New DevOps or SRE?",
      "type": "story",
      "url": "https://blog.getambassador.io/is-platform-engineering-the-new-devops-or-sre-472ed97a1885?gi=7c3c838aa208"
    }
  ]
}